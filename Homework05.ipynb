{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QshK8s21WBrf"
   },
   "source": [
    "# Homework05\n",
    "\n",
    "Some exercises with image and audio data preparation.\n",
    "\n",
    "## Goals\n",
    "\n",
    "- Even more practice with lists\n",
    "- Get familiar with pandas `DataFrames`\n",
    "- Practice dataset exploration and normalization/scaling\n",
    "- Set up a dataset for proper classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7Hf8SXUwWOho"
   },
   "source": [
    "## Setup\n",
    "\n",
    "Run the following 2 cells to import all necessary libraries and helpers for this homework"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!wget -q https://github.com/PSAM-5020-2026S-A/5020-utils/raw/main/src/audio_utils.py\n",
    "!wget -q https://github.com/PSAM-5020-2026S-A/5020-utils/raw/main/src/data_utils.py\n",
    "!wget -q https://github.com/PSAM-5020-2026S-A/5020-utils/raw/main/src/image_utils.py\n",
    "\n",
    "!wget -q https://github.com/PSAM-5020-2026S-A/Homework03/raw/main/Homework03_utils.pyc\n",
    "!wget -q https://github.com/PSAM-5020-2026S-A/Homework04/raw/main/Homework04_utils.pyc\n",
    "\n",
    "!wget -qO- https://github.com/PSAM-5020-2026S-A/5020-utils/releases/latest/download/forest-tree.tar.gz | tar xz\n",
    "!wget -qO- https://github.com/PSAM-5020-2026S-A/5020-utils/releases/latest/download/instruments.tar.gz | tar xz\n",
    "\n",
    "!wget -q https://github.com/PSAM-5020-2026S-A/5020-utils/raw/refs/heads/main/datasets/audio/hood-internet.tar.gz\n",
    "!tar -xzf ./hood-internet.tar.gz -C ./data/audio/ && rm -rf hood-internet.tar.gz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import librosa\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import random\n",
    "\n",
    "from IPython.display import Audio\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "from os import listdir\n",
    "\n",
    "from image_utils import make_image, get_pixels\n",
    "\n",
    "from Homework05_utils import AwesomeAudioClassifier, AwesomeImageClassifier\n",
    "\n",
    "AUDIO_PATH = \"./data/audio/instruments/test\"\n",
    "IMAGE_PATH = \"./data/image/forest-tree/test\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## More Image/Audio Classification\n",
    "\n",
    "We're going to re-visit the classification exercises from `Homework04` and `Homework05`.\n",
    "\n",
    "This exercise is a bit different though. In some ways it's the opposite of the previous exercises because we'll already have classification models ready to be used, but will have to normalize and standardize our dataset in order to run them. This is more representative of the type of work that goes into using real, pre-trained, ML models in the wild."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Models\n",
    "\n",
    "We have two `Awesome` models, one for audio classification (`AwesomeAudioClassifier`), and one for image classification (`AwesomeImageClassifier`).\n",
    "\n",
    "Unlike the classification models we set up for `Homework04` and `Homework05`, these models have more strict requirements about the shape and values of their input data. We can't run them on the files as they are."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Data\n",
    "\n",
    "Audio and Image files are in the `data/audio/` and `data/image/` directories respectively.\n",
    "\n",
    "We will use the `get_training_data()` from each of our classifiers to get the initial training data and labels for our audio and image files."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Features\n",
    "\n",
    "This is the challenging part.\n",
    "\n",
    "The data returned by `get_training_data()` is a representation of the content of the audio and image files, but it hasn't been processed or normalized in order to be used by the classifier models provided.\n",
    "\n",
    "We can try to create a `DataFrame` directly from those, and it might seem like it works, but if we take a look at the result we'll see some `NaN` (Not-a-Number) values in some of the columns, and if we send that to the model it will barf and complain about having `NaN`s in the data.\n",
    "\n",
    "This happens because all of the audios and images have different sizes. Hoooray !!\n",
    "\n",
    "Welcome to Machine Learning. This is probably where most of the time in any ML project is spent: cleaning up data and making sure it has the right format, size and shape that a model expects.\n",
    "\n",
    "For this exercise it won't be too hard to fix these.\n",
    "\n",
    "Let's start with the audio files since they're one-dimensional, and once we have the audio modeling working we'll come back to the image files."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Audio Data\n",
    "\n",
    "Let's run `AwesomeAudioClassifier.get_training_data()` function to get some audio data. This function returns audio data and labels from files inside a specified directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features, labels = AwesomeAudioClassifier.get_training_data(AUDIO_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Audio Features\n",
    "\n",
    "The audio data returned is actually in the frequency domain and is not samples, so even though we can't play these audio files, we can still plot this data and will have to normalize and clean it before we can run it through our classifier.\n",
    "\n",
    "Let's take a look at this data.\n",
    "\n",
    "What are the labels ? How many records do we have ? How many features do we have in each record ? Can we plot our data ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "work_cell"
    ]
   },
   "outputs": [],
   "source": [
    "# TODO: How many records ?\n",
    "\n",
    "# TODO: How many features ?\n",
    "\n",
    "# TODO: Plot some features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Looks like data !\n",
    "\n",
    "Looks like audio frequency-domain data to be more specific.\n",
    "\n",
    "If we were to follow some of the data exploration steps we saw in class we would want to put this data in a `DataFrame` in order to get calculate some of its statistical properties, and maybe scale/normalize it before we use it in a classifier model.\n",
    "\n",
    "Let's try it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_df = pd.DataFrame(features)\n",
    "features_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It looks like it works, but when we look closely at the `DataFrame`, specially if we look at the features that are further to the right, we'll see our problem: `NaN` values.\n",
    "\n",
    "As previously mentioned, this happens because the length of our features is different for each file."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fix Audio Data\n",
    "\n",
    "Let's fix this by making all of the feature lists have the same length. We can either pad the short ones or slice the longer ones to have the same length as the shortest feature list. The second option is preferable since padding would require adding information to the dataset and that might have side effects.\n",
    "\n",
    "So, we'll go through the lists of lists, create a list of lengths and find the smallest length.\n",
    "\n",
    "Then, we'll iterate through the lists of lists and slice all the feature lists to have the same length."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "work_cell"
    ]
   },
   "outputs": [],
   "source": [
    "# TODO: go through the list of features and make their lengths consistent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A `DataFrame` created using the cropped features should look more consistent now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_df = pd.DataFrame(cropped_features)\n",
    "features_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bonus: Empty features\n",
    "\n",
    "We've removed the `NaN` values, but it seems like we have a lot of columns that are all zeros or nearly all zeros, or are full of values that are close to zero, like `1e-7`.\n",
    "\n",
    "While it's not necessary, we could also remove these in order to speed up the modeling later.\n",
    "\n",
    "The `.loc[]` accessor of the `DataFrame` can be used to select columns using boolean indexing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "work_cell"
    ]
   },
   "outputs": [],
   "source": [
    "# sum of all columns\n",
    "display(features_df.sum())\n",
    "\n",
    "# columns where the sum is more than 100\n",
    "display((features_df.sum() > 100))\n",
    "\n",
    "# TODO: remove columns with no information"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run the Model\n",
    "\n",
    "Now that we have a `DataFrame` with consistent rows, we can fit and evaluate our model.\n",
    "\n",
    "The next cell runs the pre-defined classification model, fitting it with our `features_df` `DataFrame` and then reports the accuracy of our model.\n",
    "\n",
    "We just have to run it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit the classifier and report training accuracy\n",
    "AwesomeAudioClassifier.fit(features_df, labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scale / Normalize\n",
    "\n",
    "Hmmm.... it runs, but we can do better.\n",
    "\n",
    "We saw in class that normalizing/rescaling our features can help us find actual patterns in our data. It also helps models find patterns.\n",
    "\n",
    "Try scaling the `DataFrame` using either a `MinMaxScaler` or a `StandardScaler` object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "work_cell"
    ]
   },
   "outputs": [],
   "source": [
    "# TODO: scale/normalize features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run the Model Again\n",
    "\n",
    "This time with scaled data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit the classifier and report training accuracy\n",
    "AwesomeAudioClassifier.fit(features_scaled_df, labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Interpretation\n",
    "\n",
    "<span style=\"color:hotpink;\">\n",
    "Do different scaling strategies influence the prediction results ? What might that tell us about our data ?\n",
    "</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "work_cell"
    ]
   },
   "source": [
    "<span style=\"color:hotpink;\">\n",
    "EDIT THIS CELL WITH ANSWER\n",
    "</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Image Data\n",
    "\n",
    "This is a bit trickier, but only because our classifier model for images is a bit pickier. Not only do we have to ensure that all of our records have the same number of features (images have the same number of pixels), we will also have to convert the pixels into grayscale pixels.\n",
    "\n",
    "Let's start by reading the data and looking at what we get."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imgs, labels = AwesomeImageClassifier.get_training_data(IMAGE_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Image Data\n",
    "\n",
    "What did we get in the `imgs` variable ? How many records do we have ? How many features does each record/image have ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "work_cell"
    ]
   },
   "outputs": [],
   "source": [
    "# TODO: look at the imgs and labels variables and get some information about the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Image Features\n",
    "\n",
    "It seems like we have actual `PIL` image objects and their labels. \n",
    "\n",
    "This will work to our advantage because if we try to just create a `DataFrame` of the extracted pixels from these images we'll probably have a problem with missing feature values again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = []\n",
    "for img in imgs:\n",
    "  features.append(get_pixels(img))\n",
    "\n",
    "print(len(features), len(features[0]), len(features[11]))\n",
    "\n",
    "features_df = pd.DataFrame(features)\n",
    "features_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fix Images\n",
    "\n",
    "We could follow a similar approach to how we fixed the audio data, and just slice our pixel arrays to have the same length as the shortest pixel array, but that will distort our images. Try it out to see the result, but instead of taking pixels out from the end of the image, what we really have to do is change their dimensions so they all have the same `width` and `height` before we get their pixels.\n",
    "\n",
    "There are a couple of ways to achieve this:\n",
    "- Crop: use the `image.crop()` function to cut the images.\n",
    "- Resize: use `image.resize()` to stretch/squeeze the images into specific shapes.\n",
    "\n",
    "Documentation for [`crop()`](https://pillow.readthedocs.io/en/stable/reference/Image.html#PIL.Image.Image.crop) and [`resize()`](https://pillow.readthedocs.io/en/stable/reference/Image.html#PIL.Image.Image.resize).\n",
    "\n",
    "Take a look at a few images before picking a strategy and then take a look after to see what the chosen strategy does to the images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "work_cell"
    ]
   },
   "outputs": [],
   "source": [
    "# TODO: look at characteristics/dimensions of the images\n",
    "\n",
    "# TODO: go through the images and make their dimensions consistent\n",
    "\n",
    "# TODO: look at some images"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Features\n",
    "\n",
    "Now that we have images with consistent dimensions, we can extract their pixels and convert them to grayscale, so we get a nice looking `DataFrame` to send to our classifier model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "work_cell"
    ]
   },
   "outputs": [],
   "source": [
    "# TODO: calculate grayscale pixel values\n",
    "\n",
    "# TODO: look at some images with make_image()\n",
    "\n",
    "# TODO: create DataFrame"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run the Image Model\n",
    "\n",
    "Now that we have a `DataFrame` with consistent features, we can fit and evaluate our model.\n",
    "\n",
    "The next cell runs the pre-defined classification model, fitting it with our `features_df` `DataFrame` and then reports the accuracy of our model.\n",
    "\n",
    "We just have to run it (and wait a bit because it can take up to $20$ seconds for it to run)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit the classifier and report training accuracy\n",
    "AwesomeImageClassifier.fit(features_df, labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scaling / Normalizing\n",
    "\n",
    "Run the classifier model again, but this time using normalized features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "work_cell"
    ]
   },
   "outputs": [],
   "source": [
    "# TODO: create scaler object, scale data and re-run classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Interpretation\n",
    "\n",
    "<span style=\"color:hotpink;\">\n",
    "Do different scaling strategies influence the prediction results ? What might that tell us about our data ?\n",
    "</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "work_cell"
    ]
   },
   "source": [
    "<span style=\"color:hotpink;\">\n",
    "EDIT THIS CELL WITH ANSWER\n",
    "</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Shazam !\n",
    "\n",
    "Yep. Let's replicate the Shazam algorithm. It's the same type of classification task we've been looking at, but instead of classifying a piece of audio into one of $3$ classes, the algorithm has to pick which of the millions of songs the piece of audio most likely came from.\n",
    "\n",
    "We've created some simple classification models by manually extracting features that we think are important for the differentiation of our classes. This is ok if we're only dealing with a few classes, but it doesn't scale. If we had to classify audio into hundreds, or thousands, of classes, looking at graphs of relative frequencies and loudness for all classes becomes intractable.\n",
    "\n",
    "We need something a little more systematic. Something that automatically extracts meaningful numeric information from audio, independent of the type of audio.\n",
    "\n",
    "Let's look at the frequency domain again, and use the librosa library to help us extract consistent frequency information from audio files."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Librosa Feature Extraction\n",
    "\n",
    "Librosa is useful because it has many feature extraction functions to actually get meaningful frequency information from our audios files:\n",
    "\n",
    "[Librosa Reference and Documentation](https://librosa.org/doc/latest/feature.html)\n",
    "\n",
    "### Some example functions\n",
    "\n",
    "- `librosa.feature.spectral_centroid` (extracts average frequency)\n",
    "- `librosa.feature.spectral_bandwidth` (extracts frequency range)\n",
    "- `librosa.feature.spectral_rolloff(roll_percent=0.01)` (extarcts smallest frequency)\n",
    "- `librosa.feature.spectral_rolloff(roll_percent=0.99)` (extracts largest frequency)\n",
    "\n",
    "Let's load a song and see what running these looks like:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ❗️❗️ WARNING ❗️❗️\n",
    "\n",
    "The songs we'll be using are from mixtapes by [The Hood Internet](https://www.thehoodinternet.com/). They were part of a wave of _mashup_ producers in the early $2\\text{,}000\\text{s}$ that mixed \"_classic_\" american indie, pop and rap music from other time periods.\n",
    "\n",
    "We're using their mixtapes here mostly because: they're available freely on the internet, they encompass a good variety of songs and styles, and there are a good number of songs; not too few nor too many.\n",
    "\n",
    "The lyrics mostly come from the rap songs being sampled and contain references to all the things that rap artists were rapping about in the $90\\text{s}$ and $00\\text{s}$ (sex, drugs, guns, money...).\n",
    "\n",
    "We definitely don't have to listen to the songs to complete the exercise.\n",
    "\n",
    "And, I am looking for suggestions of alternative \"datasets\" for this exercise."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Back to Features Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y,sr = librosa.load(\"./data/audio/hood-internet/mixtape04/02 David Banner x Fujiya & Miyagi - Get Like Pterodactyls.mp3\")\n",
    "\n",
    "plt.plot(y)\n",
    "plt.show()\n",
    "\n",
    "display(Audio(y, rate=sr))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is how we extract the center frequency and the lowest/highest frequencies of our signal, throughout the song.\n",
    "\n",
    "Because these functions were written to work with multi-channel audio, they'll always return a list of lists. In our case, we just have a single channel, so our result will be at index $0$ of the returned list.\n",
    "\n",
    "That's why we have `sc[0]`, `smin[0]`, `smax[0]` below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# average frequency\n",
    "sc = librosa.feature.spectral_centroid(y=y)\n",
    "\n",
    "# min and max frequencies\n",
    "smin = librosa.feature.spectral_rolloff(y=y, roll_percent=0.01)\n",
    "smax = librosa.feature.spectral_rolloff(y=y, roll_percent=0.99)\n",
    "\n",
    "plt.plot(sc[0])\n",
    "plt.plot(smin[0])\n",
    "plt.plot(smax[0])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a start.\n",
    "\n",
    "The functions give us multiple values because the avg/min/max frequency changes over time, but we can average the `spectral_centroid` and the two `spectral_rolloff` frequencies to get $3$ values that represent our entire audio file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc_val = sum(sc[0]) / len(sc[0])\n",
    "smin_val = sum(smin[0]) / len(smin[0])\n",
    "smax_val = sum(smax[0]) / len(smax[0])\n",
    "\n",
    "plt.plot([0, len(sc[0])], [sc_val, sc_val])\n",
    "plt.plot([0, len(smin[0])], [smin_val, smin_val])\n",
    "plt.plot([0, len(smax[0])], [smax_val, smax_val])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Not bad.\n",
    "\n",
    "### MFCCs\n",
    "\n",
    "But, we can extend this idea with Mel-Frequency Cepstral Coefficients (MFCC). MFCCs are a compact, numerical representation of an audio signal's frequency characteristics, that divide an audio frequency range into bands based on human hearing sensitivities. So, instead of just $3$ frequencies, like we got above, the total frequency range is divided into $10$ to $20$ values that represent our audio signal.\n",
    "\n",
    "And of course `librosa` has an `mfcc()` function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mfccs = librosa.feature.mfcc(y=y, n_mfcc=10)\n",
    "print(mfccs.shape)\n",
    "\n",
    "for vs in mfccs:\n",
    "  plt.plot(vs)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Average MFCCs\n",
    "\n",
    "Like the `spectral_centroid()` function, the `mfcc()` function gives us multiple values for each frequency channel/band, since these change over time as the audio changes. We'll assume that one value per frequency band is enough and average them over time, so we end up with only $10$ numbers to represent our audio."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mfccs_avg = mfccs.mean(axis=1)\n",
    "print(mfccs_avg.shape)\n",
    "\n",
    "for vs in mfccs_avg:\n",
    "  plt.plot([0,100], [vs, vs])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Back to Shazam\n",
    "\n",
    "This is what's behind the [Shazam algorithm](https://www.youtube.com/watch?v=b6xeOLjeKs0): a large database of songs, where each song is described with only a few features so they can be organized, searched, queried and compared very very quickly.\n",
    "\n",
    "Another [video](https://www.youtube.com/watch?v=kMNSAhsyiDg) reference.\n",
    "\n",
    "Let's implement something like this.\n",
    "\n",
    "We'll focus on a single album, with about $30$ songs, but the same strategy (with minor modifications) will work for datasets of many more songs.\n",
    "\n",
    "Here's the outline of what we have to do:\n",
    "\n",
    "1. Extract MFCC features from all songs in our data base (single album)\n",
    "2. Create a dataset of song features\n",
    "3. Given an input sound clip: extract its MFCC features\n",
    "4. Find closest song in database by feature distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First, let's grab all audio file names from our album directory\n",
    "\n",
    "TRACKS_DIR = \"./data/audio/hood-internet/mixtape04\"\n",
    "fnames = sorted([f for f in listdir(TRACKS_DIR) if f.endswith(\"mp3\")])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now we define a helper function that:\n",
    "#   given a list of samples, \n",
    "#   returns a dictionary of extracted mfcc values\n",
    "\n",
    "def get_features(y, n_mfcc=20):\n",
    "  mfeats = {}\n",
    "  mfccs = librosa.feature.mfcc(y=y, n_mfcc=n_mfcc).mean(axis=1)\n",
    "  for idx,fcc in enumerate(mfccs):\n",
    "    mfeats[f\"mfcc{idx}\"] = float(fcc)\n",
    "  return mfeats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "work_cell"
    ]
   },
   "outputs": [],
   "source": [
    "# TODO: run the get_features() function in one of the loaded files\n",
    "# TODO: check the output and see if it makes sense"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Extract MFCC features from all songs\n",
    "\n",
    "It takes about $20$ seconds to extract features for all songs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "work_cell"
    ]
   },
   "outputs": [],
   "source": [
    "# TODO: Run the function on all tracks and extract their average MFCCs\n",
    "\n",
    "# TODO: iterate over the list of filenames, open each file,\n",
    "#       extract mfcc features and append result to a list\n",
    "\n",
    "track_feats = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Create `DataFrame` of song features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "work_cell"
    ]
   },
   "outputs": [],
   "source": [
    "# TODO: put results in pandas DataFrame using the from_records() function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Given an input sound clip: extract its MFCC features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's open track 02 (index 1) and get 5 seconds of audio from the center of the song\n",
    "\n",
    "# Track 02\n",
    "fpath = f\"{TRACKS_DIR}/{fnames[1]}\"\n",
    "y_in, sr_in = librosa.load(fpath)\n",
    "\n",
    "# get 5 seconds from middle of song\n",
    "midx = int(len(y_in) // 2)\n",
    "half_win = int(2.5 * sr_in)\n",
    "samples_5sec = y_in[midx - half_win : midx + half_win]\n",
    "\n",
    "feat_in = [get_features(samples_5sec)]\n",
    "\n",
    "# And create DataFrame with MFCC features for the 5-second clip\n",
    "in_df = pd.DataFrame.from_records(feat_in)\n",
    "in_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Find closest song in database by feature distance\n",
    "\n",
    "Because both our dataset of song features and the features from the input clip we want to find are in `DataFrames`, we can do some `DataFrame` math to quickly calculate the difference between the input clip features and features from all of the songs in our song database. We'll use this to find the row in our dataset that is closest to the input audio.\n",
    "\n",
    "We can start the process with this line of code:\n",
    "```py\n",
    "dists = (tracks_df - in_df.iloc[0]).abs().sum(axis=1)\n",
    "```\n",
    "\n",
    "Pretty intense. Let's break it down:\n",
    "\n",
    "- `tracks_df - in_df.iloc[0]`: this subtracts the first (and only) row of the input `DataFrame` from every row of the song dataset. The result is another `DataFrame` with $34$ rows and $20$ columns, where the cells don't hold MFCC values anymore, but the difference between the values in the input clip and the values from the songs in the dataset.\n",
    "\n",
    "- `.abs()`: the absolute value of the difference. After the subtraction, some of the values might be negative, but we only care about their magnitude; how different they are, and not which is larger/smaller. The result is another `DataFrame` of the same size as the previous, but now all values are positive.\n",
    "\n",
    "- `.sum(axis=1)`: this sums the differences across the columns. The result is a `DataFrame` with $34$ rows and a single column with a value that represents how different each of the songs in the dataset is from the input clip. The larger the number, the more different the song is.\n",
    "\n",
    "What we still have to do is find out which row has the shortest distance to our input clip.\n",
    "\n",
    "We can do this in a variety of ways.\n",
    "\n",
    "One possibility is to just loop over the values and use $2$ variables to keep track of the smallest difference value and the index of where that difference is.\n",
    "\n",
    "A more general approach is to turn our `DataFrame` into a list of pairs of row indexes and difference values:\n",
    "\n",
    "```py\n",
    "[[0, difference], [1, difference], [2, difference], ...]\n",
    "```\n",
    "\n",
    "and sort by the difference values. As we'll see below, this approach has advantages.\n",
    "\n",
    "We can turn our single-column `dists` `DataFrame` into a list like this using the `.to_dict()` function followed by the `.items()`.\n",
    "\n",
    "The `.to_dict()` function turns the single-column `DataFrame` into a `Python` dictionary, like this:\n",
    "```py\n",
    "{\n",
    "  0: difference,\n",
    "  1: difference,\n",
    "  2: difference,\n",
    "  ...\n",
    "}\n",
    "```\n",
    "\n",
    "And the `items()` function is the regular `Python` dictionary that gives us dictionary (`key`,`value`) pairs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dists = (tracks_df - in_df.iloc[0]).abs().sum(axis=1)\n",
    "idx_dist_pairs = dists.to_dict().items()\n",
    "\n",
    "# break these down into multiple steps and see if each step makes sense\n",
    "(tracks_df - in_df.iloc[0]) # add abs, sum, to_dict and items one at a time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We still have to sort our distance pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "work_cell"
    ]
   },
   "outputs": [],
   "source": [
    "# TODO: sort the idx_dist_pairs list by the difference values\n",
    "# TODO: after sorting, look at the first item and check if the index matches the index of the input song"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Evaluate\n",
    "\n",
    "We should calculate how often our Shazam algorithm is right.\n",
    "\n",
    "This can be done by implementing the following set of steps:\n",
    "1. Initialize a counter to keep track of number of correct predictions\n",
    "2. For $N$ predictions (where $N$ can be $10$, $100$, $1000$, etc ...)\n",
    "3. Pick a random track from the album\n",
    "4. Pick a random $5$ second clip from the random track\n",
    "5. Extract features, compare to features in `tracks_df` and predict which track it is\n",
    "6. If prediction correct, increment correct predictions counter\n",
    "\n",
    "Alternatively, instead of only incrementing the correct predictions counter when the correct track is the very first on the list of possible tracks, we can increment the counter if the correct track is in the $\\text{top-}3$ predictions. This is the $\\text{top-}3$ accuracy of our classifier, which, instead of being a binary measurement of right/wrong, also gives us a bit of credit for being \"close\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "work_cell"
    ]
   },
   "outputs": [],
   "source": [
    "# TODO: write a function called random5() that\n",
    "#       receives an audio file path, extracts MFCC features from \n",
    "#       a random 5-second portion of the audio and returns a DataFrame with the features\n",
    "\n",
    "def random5(fpath):\n",
    "  # TODO: implement here. random.randrange() can help\n",
    "  return pd.DataFrame.from_records([])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll also need a function for comparing the `DataFrame` of songs and the `DataFrame` of the input clip.\n",
    "\n",
    "We just have to wrap the mega-line of `Pandas` and the sorting from above in a function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "work_cell"
    ]
   },
   "outputs": [],
   "source": [
    "# TODO: implement comparison function\n",
    "def compare_predict(tracks_df, in_df):\n",
    "  # implement here\n",
    "  return []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we use the above defined functions to run some tests.\n",
    "\n",
    "We'll want to run $200$ to $500$ tests, but we can start with $10$ or so just to make sure the code works."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "work_cell"
    ]
   },
   "outputs": [],
   "source": [
    "# TODO: pick a random song\n",
    "# TODO: extract features from random 5-second clip of song\n",
    "# TODO: compare, and keep track of correct predictions\n",
    "# TODO: repeat\n",
    "# TODO: print accuracy (number of correct predictions divided by total tests)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Improve the Algorithm ?\n",
    "\n",
    "Not bad.\n",
    "\n",
    "(Re)Watch the Shazam video [here](https://www.youtube.com/watch?v=b6xeOLjeKs0). What's the one big difference between their algorithm and ours ?\n",
    "\n",
    "(You don't have to improve the code. Just think about what could be some potential next steps.)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyPxe2qYxIG7EblrvD1C4Pmv",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "5020",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
